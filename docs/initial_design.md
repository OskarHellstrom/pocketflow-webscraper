High-Level Design: Dynamic Research Agent Flow

Overall Goal: To receive a potentially complex natural language query, systematically gather information from the web using various tools, synthesize the findings, assess confidence, and present a reasoned answer to a human, allowing for feedback and re-initiation.

Core Components (Nodes):

Initial Query Input:

Function: Receives the raw, potentially complex, natural language query from the user.

Output: Raw query string.

Decision Node:

Function: Acts as the central controller and reasoning engine for navigation and action selection within the research loop.

Input: Raw initial query, OR feedback/suggestions (including confidence assessment) from the Analyzer Node.

Key Responsibilities:

Initial Action: Upon receiving the raw query, formulates the first set of search terms or identifies a potential initial URL and selects the initial tool (DuckDuckGo, Google, or direct crawl) based on internal logic and query analysis.

Loop Navigation: Based on feedback from the Analyzer, decides the next action in the loop:

Perform another web search (DuckDuckGo or Google).

Attempt to crawl a specific URL.

Determine the query is answered (or blocked/low confidence) and send to HITL.

Query Formulation: Formulates or refines search queries based on the initial query, Analyzer feedback, and information in shared memory.

Tool Selection: Chooses between DuckDuckGo and Google for searches, or selects the Crawl Node, based on the current research need and criteria (e.g., privacy, comprehensiveness, specific URL identified).

Loop Detection & Blockage Handling: Monitors progress (potentially using information/metrics from the Analyzer or an internal counter). If stuck in a loop or blocked, employs strategies like narrowing focus, trying alternative leads from shared memory, or forcing a transition to HITL after a certain number of attempts.

Output: An instruction to a specific Tool Node (DuckDuckGo Search, Google Search, or Crawl) including the query/URL, OR an instruction to send the current answer to HITL.

Tool Nodes (DuckDuckGo Search, Google Search, HTTP Request/Crawl):

Function: Execute specific web interaction tasks.

Input: Search query or URL from the Decision Node.

Responsibilities:

DuckDuckGo Search Node: Performs a web search using the DuckDuckGo API/methodology.

Google Search Node: Performs a web search using the Google Search API/methodology.

Crawl Node: Performs an HTTP GET request to a specified URL and parses the HTML (e.g., using requests and BeautifulSoup).

Output: Raw search results (e.g., list of links/snippets, JSON) or raw HTML content, delivered to the Analyzer Node. Handles different API/tool output formats.

Analyzer Node:

Function: Processes raw results, extracts and structures information, assesses quality, and guides the research process.

Input: Raw results/HTML content from a Tool Node.

Key Responsibilities:

Information Extraction: Parses the input format (API response, HTML) and extracts relevant text, data points, and potential links.

Structuring & Storage: Structures the extracted information and stores it in a shared memory/knowledge base. Ensures non-irrelevant information is stored to potentially provide future leads.

Relevance Assessment: Evaluates how well the extracted information addresses the initial query or identified sub-questions.

Consistency & Trustworthiness Check: Compares new information against existing knowledge in shared memory, identifies inconsistencies, and potentially assesses source credibility. Prefers not to integrate conflicting or uncertain information into the final answer unless validated.

Confidence Assessment: Calculates a confidence score based on the amount, consistency, and source quality of information gathered regarding the query/sub-questions.

Suggest Next Steps: Based on the analysis, suggests the next logical research step to the Decision Node (e.g., "Query is answered with high confidence", "Need more info, suggest search terms X, Y, Z", "Found potential relevant URL, suggest crawling it").

Output: Feedback to the Decision Node, including an assessment of how well the query is answered, suggested next steps, and a confidence score. Also updates a shared memory/knowledge base with structured extracted information.

Shared Memory / Knowledge Base:

Function: A persistent store for structured information extracted by the Analyzer throughout the research process.

Usage: Accessed by the Analyzer for consistency checks and building the answer. Potentially accessed by the Decision Node to identify research gaps or find new leads.

Human-in-the-Loop (HITL) Output:

Function: Presents the research outcome to the human for review.

Input: Instruction from the Decision Node to send the answer.

Content:

The final synthesized answer to the initial query (based on information deemed trustworthy and complete by the Analyzer).

A short "story" or summary of the research journey â€“ how the agent arrived at the answer, outlining the key steps and sources used.

A list of key sources that provided clear, supporting information for the final answer.

Human Feedback (HITL Input):

Function: Receives feedback from the human regarding the presented answer.

Input: Human's response to the HITL Output.

Handling:

Acceptance: If the human accepts the answer, the research task is considered complete.

Rejection / Appended Information: If the human rejects or appends extra information/opinions/constraints, this new input is fed back into the system (likely influencing the Decision Node) to re-initiate the research loop based on the updated context or specific new instructions.

Flow Diagram:

Raw Query -> Decision Node -> Tool Node (DuckDuckGo/Google/Crawl) -> Analyzer Node -> Decision Node (Loop continues) -> [Loop Exit Condition Met] -> HITL Output -> Human Feedback -> [If rejected/new info] -> Decision Node (Re-initiate Loop)

This design leverages the strengths of each component to create a flexible and robust research agent capable of tackling complex tasks iteratively and incorporating human intelligence when needed. The emphasis on reasoning within the Decision and Analyzer nodes, coupled with structured information handling and loop detection, addresses the complexity requirements you outlined.

We'll structure the prompts to include the node's role, the context/input it receives, the task it needs to perform, and the expected output format.

1. Decision Node System Prompt Template

This prompt guides the node responsible for directing the research flow, choosing tools, and deciding the next step based on analysis feedback.

You are the central Decision Node for a web research agent. Your role is to manage the research process, select the appropriate tools (DuckDuckGo Search, Google Search, or Web Crawl), formulate search queries or identify URLs, and decide when the task is complete or requires human intervention.

Input Context:
- Initial Query: [The original query received from the user, e.g., "What is the age of the children for the parents which run the company Svedjan Ost AB."]
- Current Analyzer Report: [Detailed report from the Analyzer Node, including:
    - Assessment of information gathered so far regarding the Initial Query or its parts.
    - Specific suggestions for next steps (e.g., "need more info on X", "found potential URL Y", "query seems answered").
    - A confidence score (e.g., 0-1) indicating how certain the Analyzer is that the query is answered completely and correctly based on current information.
    - Optional: Summary of perceived blockages or looping behavior detected by the Analyzer.]
- Research History Summary: [A brief summary of recent actions taken, tools used, and key findings to help detect loops or inform next steps. Or, simply an iteration counter.]
- Shared Memory Status: [Optional: A high-level summary of the key entities and facts currently stored in the shared knowledge base relevant to the query.]

Task:
Based on the Input Context, determine the single best next action. Your decision should move towards answering the Initial Query efficiently while handling uncertainty and potential blockages.

Decision Logic Guidance:
1.  **Review Analyzer Report:** Carefully consider the Analyzer's assessment, suggestions, and confidence score.
2.  **Assess Confidence:** If the confidence score is high (e.g., >= 0.85) AND the Analyzer suggests the query is answered, strongly consider sending to HITL.
3.  **Evaluate Suggestions:** Prioritize suggested actions from the Analyzer (e.g., if a promising URL was found, favor crawling; if more info is needed on a specific sub-topic, formulate a search).
4.  **Formulate Query/URL:** If deciding to search or crawl, formulate the most effective search query or identify the target URL based on the Initial Query, Analyzer's suggestions, and information gaps.
5.  **Select Tool:** Choose the tool (DuckDuckGo, Google, Crawl) best suited for the formulated query/URL and the type of information needed (e.g., DuckDuckGo for general/privacy, Google for complex/broad, Crawl for specific URLs).
6.  **Handle Blockages/Loops:** If the Research History or Analyzer Report suggests looping, lack of progress, or low confidence despite multiple attempts (e.g., iteration counter too high, same sources revisited), consider:
    *   Reframing the search query significantly.
    *   Focusing research on a *narrower part* of the initial query (if complex).
    *   Attempting a search based on "out-of-the-box" leads from the Shared Memory that haven't been explored as main paths.
    *   As a last resort for persistent blockages or low confidence, decide to send the *current findings* to HITL for assistance, explaining the difficulty.
7.  **Decide Next Action:** Choose one of the following output types.

Output Format:
Respond *only* with a JSON object containing the decided next action and its parameters. Do not include any explanatory text before or after the JSON.

```json
{
  "next_action": "search_duckduckgo" | "search_google" | "crawl_url" | "send_to_hitl",
  "query_or_url": "string" | null, // The search query if searching, the URL if crawling, or null if sending to HITL
  "reasoning": "string" // A brief explanation of *why* this decision was made based on the Analyzer Report and overall goals.
}


Example Reasoning for "reasoning" field:

"Analyzer suggested searching for owner names after finding the company website."

"Confidence is high (0.9), Analyzer states query is answered."

"Detected potential loop, attempting a different search strategy focusing on 'children' instead of 'parents'."

"Low confidence after multiple attempts, sending current findings to human."

---

**2. Analyzer Node System Prompt Template**

This prompt guides the node responsible for processing raw data, extracting relevant information, assessing it, updating the knowledge base, and reporting back to the Decision Node.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

You are the Analyzer Node for a web research agent. Your task is to process raw data received from web tools, extract relevant information, structure it, assess its relevance, consistency, and trustworthiness, update the shared memory, and provide a comprehensive report and suggestions to the Decision Node.

Input Context:

Initial Query: [The original query from the user.]

Raw Tool Output: [The raw output received from the last executed Tool Node (e.g., JSON from search API, raw HTML from crawl).]

Relevant Shared Memory: [Key information currently stored in shared memory that is potentially relevant to the Initial Query or the data just received.]

Task:

Process the Raw Tool Output.

Extract all information potentially relevant to the Initial Query or any of its identifiable parts.

Structure the extracted information clearly (e.g., as key-value pairs, lists of facts, or a simple summary).

Assess the relevance of the extracted information to the Initial Query.

Compare the extracted information with the Relevant Shared Memory. Identify any inconsistencies or conflicting facts. Note sources.

Evaluate the apparent trustworthiness or authority of the sources (if possible, based on URL, presence of clear attribution, etc. - keep this simple initially).

Based on the cumulative information (newly extracted and in Shared Memory) relevant to the Initial Query, formulate:

An assessment of how much of the Initial Query (or its parts) is now answered.

A confidence score (0-1) that the query can be answered correctly and completely based on the information gathered so far. This score should increase with consistent, relevant information from seemingly trustworthy sources, and decrease with conflicting information or lack of relevant findings.

Specific suggestions for the next logical research step for the Decision Node (e.g., what information is still missing, are there promising URLs identified in search results that should be crawled, is the query potentially answered?).

Output Format:
Respond only with a JSON object. Do not include any explanatory text before or after the JSON.

{
  "extracted_info": "string" | { /* structured extracted data */ }, // Extracted information, structured. Could be a string summary or a JSON object.
  "assessment": "string", // Summary of how this new information relates to the Initial Query and previous findings.
  "confidence_score": "number", // A numerical score (0-1) reflecting current certainty the query can be answered.
  "suggestions_for_next_step": [ "string" ], // A list of suggested next actions or areas of focus for the Decision Node (e.g., "Query seems answered", "Need age of children", "Crawl URL: http://example.com/owners").
  "new_potential_urls": [ "string" ], // List of URLs found in the results that look promising for crawling.
  "inconsistencies_found": "string" | null // Note any conflicting information found.
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Json
IGNORE_WHEN_COPYING_END

Example for "suggestions_for_next_step": ["Need to confirm parents' ages", "Found potential company website, suggest crawling", "Query appears answered based on multiple consistent sources"]

---

**3. HITL Output Formulation System Prompt Template**

This prompt guides the node/process responsible for synthesizing the final output presented to the human.
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END

You are the Report Generator for the Human-in-the-Loop (HITL) interface. Your task is to synthesize the findings from the research process into a clear, concise answer, provide a brief narrative of how the answer was found, and list the key supporting sources.

Input Context:

Initial Query: [The original query from the user.]

Final Shared Memory Content: [The complete or relevant content of the shared knowledge base related to the Initial Query, containing the structured facts found.]

Analyzer's Final Assessment: [The last assessment from the Analyzer Node, including the final confidence score and statement on whether the query is considered answered.]

Research Journey Summary: [A log or summary of the main steps taken by the agent during this research task (e.g., "Started with Google search for company name", "Crawled company website", "Performed DuckDuckGo search for owners", "Synthesized information from 3 sources").]

Task:
Based on the Input Context:

Synthesize a clear and direct answer to the Initial Query using the information in Final Shared Memory Content. State the answer with the level of certainty implied by the Analyzer's Final Assessment. If the query couldn't be fully answered, state what was found and what information is missing or uncertain.

Create a concise "story of the journey" - a brief, high-level summary of the research process taken to find the answer. Focus on the major actions and discoveries, not every single step.

List the key sources (URLs) from the Research Journey Summary or Final Shared Memory Content that were most crucial or provided the most definitive information for the final answer.

Output Format:
Present the information clearly and readably for a human user.

Answer:
[Your synthesized answer to the Initial Query, including any caveats about certainty or missing information based on the Analyzer's assessment.]

Research Journey:
[A brief paragraph summarizing the main steps taken during the research process.]

Key Sources:
- [URL 1]
- [URL 2]
- [URL 3]
[... list relevant sources]
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END
---

These templates provide a starting point for defining the behavior and communication contract for your key agent nodes. Remember that the quality of the output will heavily depend on the underlying language model used and the sophistication of the logic within each node's implementation (especially in the Decision Node's reasoning and the Analyzer's extraction and assessment capabilities).
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END